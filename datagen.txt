Python 3, numpy, pandas, sklearn, keras
отладка кода Anaconda, Spider
среда для подбора параметров модели - Google Colaboratory (GPU)

Генерация данных:

1. Строю функцию для выработки значения ответа (входы параметры конического сечения А, В, С). При этом формируется 2 значения выхода на каждый набор входов. Первый ответ = значению дискриминанта, а второй метка класса (-1, 0, 1) - эллипс, парабола, гипербола (ConicSectionType(x)):
2. Определяю диапазон изменения параметров А, В,С (+\-1000) и число примеров по классам (5000)
3. Строю 2/3 выборок за счет случайной равномерной генерации во всем диапазоне значений и определяю для этих комбинаций значение дискриминанта и класс объекта. Очевидно, что объектов для класса 0 так не построить. Поэтому примерно половина будет класса -1 и половина 1.
4. Строю еще 1/3 примеров для класса (0) за счет решения уравнения b = +/- (4 * a * c)^0.5. При этом а строю, как линейную интерполяцию от 0 до 20, а с - как интерполяцию от 20 до 0. При этом учитываю, что половина комбинаций (а, с) должна быть со знаками "-", а половина с "+" . Ждя этого строю случайно равномернкю маску (из -1 и 1) размера (1, размер класса). Кроме этого, получаемые по итогам обсчета b =(a * c * 4)^0.5 всегда больше 0, и на эту операцию накидываю еще одну случайную маску из -1 и 1.
5. значения выхода для дискриминанта и классификатора для этой части модели равны 0.
6. проверяю, получилась ли модель (вывод на график)
7. Соединяю части выборки в одну модель и сохраняю 

8. Перехожу к формированию модели по части для тренировки (train.csv).(sklearn, Keras)
9. Формирую предобработку входов модели (прямые значения, Нормализованные значения, полиномизация прямых и нормализованных значений с порядком 2 и 3). Формируется 6 моделей входных признаков.
10 Формирую список регрессоров (буду учить на значения дискриминанта) и классификаторов (буду уить на класс объекта) для предварительного обучения
11. Обучаю и сравниваю ссредние ошибки по моделям решателей и моделям данных.
12. Регрессия (Случайный лес)  работает значительно лучше всех и лучшая модель данных (полиномизация с порядком 2(а кто сомневался))
13. Тренирую регрессор и определяю его параметры (и как в отборе моделей работал неплохо) и строю решатель. Для этого нужно перейти от регрессии к классификатору.
14. решатель имеет параметр порога. т.е. (-1 и 1 можно построить автоматически используя условие предиктор>0 - класс 1, предиктор < 0 - rkfcc -1, а класс 0 - это некоторая близость к 0, но никогда не == 0) нужно назначить достаточно маленькое значение предиктора(регрессора), которое отвечает за объекты класса 0 из обучающей выборки. Использую оценку среднего отклонения предиктора для этих объектов (около 650 для моего обучения) и определяю класс 0 как все, что по абсолютному значению предиктора меньше  порога.
15. Проверяю в тесте. (от раза к разу 0,06 - 0.08% ошибок по тесту)


